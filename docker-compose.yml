version: "3.8"

services:
  converter:
    image: ghcr.io/janhq/triton_tensorrt_llm:engine_build_89_90_5955b8afbad2ddcc3156202b16c567e94c52248f
    command: /tmp/tensorrt_llm_converter.sh
    volumes:
      - ./engine:/engine
      - ./tokenizer:/tokenizer
      - ./checkpoint:/checkpoint
      - ./tensorrt_llm_converter.sh:/tmp/tensorrt_llm_converter.sh
    env_file:
      - .env
    working_dir: /app/tensorrt_llm
    tmpfs:
      - /tmp:exec
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  triton:
    image: ghcr.io/janhq/triton_tensorrt_llm:engine_build_89_90_41fe3a6a9daa12c64403e084298c6169b07d489d
    command: /tmp/triton_init.sh
    env_file:
      - .env
    ports:
      - "8000"
      - "8001"
      - "8002"
    volumes:
      - ./engine:/engine
      - ./triton:/triton
      - ./tensorrtllm_backend/tools:/app/tools
      - ./tensorrtllm_backend/all_models/inflight_batcher_llm:/model
      - ./tokenizer:/tokenizer
      - ./triton_init.sh:/tmp/triton_init.sh
    working_dir: /app/
    depends_on:
      converter:
        condition: service_completed_successfully
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  proxy:
    image: ghcr.io/janhq/triton_tensorrt_llm:proxy_openai_2ec5869dc61362118ebef7f097e00d2da0cc0f69
    command:
      [
        "--host",
        "0.0.0.0",
        "--port",
        "3000",
        "--triton-endpoint",
        "http://triton:8001",
      ]
    ports:
      - "3000:3000"
    depends_on:
      - triton
    restart: always
